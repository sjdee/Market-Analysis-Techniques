{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjdee/Market-Analysis-Techniques/blob/master/RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9_r0N63RFz",
        "colab_type": "text"
      },
      "source": [
        "To remove change_5day and change_10day  modify features to 5:-12 below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrbSgcsInm9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://towardsdatascience.com/data-pre-processing-techniques-you-should-know-8954662716d6\n",
        "\n",
        "# Standardizing the features\n",
        "# df['Vamount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "# df['Vtime'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "# df = df.drop(['Time','Amount'], axis = 1)\n",
        "# df.head()\n",
        "import time\n",
        "from time import gmtime, strftime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def run_classifier (pDf,minify_data,less_columns,print_data):  \n",
        "\n",
        "  \n",
        "  df = pDf\n",
        "  \n",
        "  if(less_columns==True):\n",
        "    df = df.loc[:,['BEST_ANALYST_RATING', 'RETURN_ON_ASSET', 'BEST_TARGET_PRICE', 'CUR_MKT_CAP', 'SHORT_INT', 'TOT_BUY_REC', 'TOT_SELL_REC', 'day1','day2', 'day3', 'day4','day5', 'day6','day7', 'day8', 'day9', 'day10']]\n",
        "    df.dropna(inplace=True)\n",
        "    features = df.iloc[:,:-10]\n",
        "    labels = df.iloc[:,-10:]\n",
        "    \n",
        "  else:\n",
        "    df.dropna(inplace=True)\n",
        "    features = df.iloc[:,5:-12]\n",
        "    labels = df.iloc[:,-10:]\n",
        "       \n",
        "    \n",
        "  report_data = []\n",
        "\n",
        "\n",
        "  for i in range(len(labels.columns)):\n",
        "    # specify the feature set, target set, the test size and random_state to select records randomly\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels.iloc[:,i], test_size=0.3,random_state=0) \n",
        "\n",
        "\n",
        "    # Scaling values in the feature set\n",
        "    scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
        "    X_train = scaling.transform(X_train)\n",
        "    X_test = scaling.transform(X_test)\n",
        "\n",
        "\n",
        "    start = time.time()\n",
        "    \n",
        "    # Create a random forest Classifier\n",
        "    clf = RandomForestClassifier(n_jobs=2, random_state=0,n_estimators=10)\n",
        "\n",
        "    # Train the model using the training sets\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the response for test dataset\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    \n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    f1_scores =[] \n",
        "    f1_scores.insert(0, metrics.f1_score(y_test, y_pred, average='micro'))\n",
        "    f1_scores.insert(1, metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    if(print_data==True):\n",
        "      print(\"day number \"+ str(i)+ \" classified and took \" + str((end - start)) + \" seconds\")\n",
        "      print(accuracy)\n",
        "      print(f1_scores[0])\n",
        "      print(f1_scores[1])\n",
        "      print(report)\n",
        "    else:\n",
        "      accumulate_data(report_data,report,accuracy,labels.iloc[:,i].name,\"Random Forest\",minify_data,f1_scores)\n",
        "    \n",
        "  return report_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vf7kD_UMJKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accumulate_data(report_data,report,accuracy,day_name,model_name,minify_data,f1_scores):\n",
        "      \n",
        "  print(day_name)\n",
        "  print(report)\n",
        "  \n",
        "  if(minify_data == True):\n",
        "\n",
        "    row = {}\n",
        "    row['day'] = day_name.replace(\"day\", \"\") \n",
        "    row['accuracy'] = accuracy\n",
        "#     row['f1score_macro'] = f1_scores[1]\n",
        "    row['f1score_micro'] = f1_scores[0]\n",
        "    row['f1score_weigthed'] = f1_scores[1]\n",
        "    # row['model'] = model_name\n",
        "\n",
        "    # unravel report for the given day       \n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-5]:\n",
        "\n",
        "      row_data = line.split('     ')\n",
        "\n",
        "      # update recall for sell\n",
        "      if(float(row_data[1])==0.0):\n",
        "        row['sell_recall']= float(row_data[3])\n",
        "      # update precison for buy\n",
        "      if(float(row_data[1])==2.0):\n",
        "        row['buy_precison']= float(row_data[2])\n",
        "\n",
        "    report_data.append(row)\n",
        "\n",
        "\n",
        "  else:    \n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-5]:\n",
        "        row = {}\n",
        "        row_data = line.split('     ')\n",
        "        row['model'] = model_name\n",
        "        row['accuracy'] = accuracy\n",
        "        row['day'] = labels.iloc[:,i].name\n",
        "        row['class'] = row_data[1]\n",
        "        row['precision'] = float(row_data[2])\n",
        "        row['recall'] = float(row_data[3])\n",
        "        row['f1_score'] = float(row_data[4])\n",
        "        row['support'] = float(row_data[5])\n",
        "        report_data.append(row)\n",
        "\n",
        "  return report_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrapAgQroqkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " sectors = ['Real Estate']\n",
        "# sectors = ['Communication Services', 'Consumer Discretionary','Consumer Staples', 'Energy', 'Financials', 'Health Care', 'Industrials', 'Information Technology', 'Materials', 'Real Estate', 'Utilities']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN-Rrzf9kieR",
        "colab_type": "code",
        "outputId": "133fca43-1318-4e32-9758-4879f8f80422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# https://towardsdatascience.com/multivariate-time-series-forecasting-using-random-forest-2372f3ecbad1\n",
        "# https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "file_id = '18pa4iuqvz2SX5RYrUdn09bDU8eNm2hqI'\n",
        "\n",
        "# 2. Load a file by ID \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('sp500_transformation_input.xlsx')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 3.4MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxZ_8hekq9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_all = pd.read_excel('sp500_transformation_input.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwjQ0DNwGViZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_all\n",
        "df.set_index('Sector', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAsH8U_ZQGr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_each_sector = False\n",
        "less_columns = True\n",
        "minify = True\n",
        "print_data = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr52gSpqLf3p",
        "colab_type": "code",
        "outputId": "28e9a419-644c-4667-fae0-f238b02e5367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1762
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "if(check_each_sector == True):\n",
        "   \n",
        "  for k in range(len(sectors)):\n",
        "    print('Running classifier by sector.')\n",
        "\n",
        "    df_sectorised = df.loc[sectors[k]]\n",
        "    \n",
        "    report = run_classifier(df_sectorised,minify,less_columns,print_data)\n",
        "\n",
        "    \n",
        "    dataframe = pd.DataFrame.from_dict(report)\n",
        "    file_name = sectors[k]+'RandomForest_classification_report.csv'\n",
        "    dataframe.to_csv(file_name, index = False)\n",
        "    files.download(file_name)\n",
        "    \n",
        "else:\n",
        "  print('Running classifier on all.')\n",
        "  \n",
        "  report = run_classifier(df,minify,less_columns,print_data)\n",
        "\n",
        "  dataframe = pd.DataFrame.from_dict(report)\n",
        "  dataframe.to_csv('RandomForest_classification_report.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running classifier on all.\n",
            "day1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.50      0.49     52535\n",
            "         2.0       0.64      0.62      0.63     73687\n",
            "\n",
            "    accuracy                           0.57    126222\n",
            "   macro avg       0.56      0.56      0.56    126222\n",
            "weighted avg       0.57      0.57      0.57    126222\n",
            "\n",
            "day2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82     91528\n",
            "         2.0       0.50      0.36      0.42     34694\n",
            "\n",
            "    accuracy                           0.72    126222\n",
            "   macro avg       0.64      0.61      0.62    126222\n",
            "weighted avg       0.70      0.72      0.71    126222\n",
            "\n",
            "day3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82     86012\n",
            "         2.0       0.62      0.50      0.55     40210\n",
            "\n",
            "    accuracy                           0.74    126222\n",
            "   macro avg       0.70      0.68      0.69    126222\n",
            "weighted avg       0.73      0.74      0.73    126222\n",
            "\n",
            "day4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83     82085\n",
            "         2.0       0.69      0.60      0.64     44137\n",
            "\n",
            "    accuracy                           0.77    126222\n",
            "   macro avg       0.75      0.73      0.73    126222\n",
            "weighted avg       0.76      0.77      0.76    126222\n",
            "\n",
            "day5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.86      0.84     79267\n",
            "         2.0       0.74      0.67      0.70     46955\n",
            "\n",
            "    accuracy                           0.79    126222\n",
            "   macro avg       0.78      0.76      0.77    126222\n",
            "weighted avg       0.79      0.79      0.79    126222\n",
            "\n",
            "day6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.84     77271\n",
            "         2.0       0.77      0.71      0.74     48951\n",
            "\n",
            "    accuracy                           0.80    126222\n",
            "   macro avg       0.80      0.79      0.79    126222\n",
            "weighted avg       0.80      0.80      0.80    126222\n",
            "\n",
            "day7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.87      0.85     75466\n",
            "         2.0       0.80      0.74      0.77     50756\n",
            "\n",
            "    accuracy                           0.82    126222\n",
            "   macro avg       0.81      0.81      0.81    126222\n",
            "weighted avg       0.82      0.82      0.82    126222\n",
            "\n",
            "day8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.88      0.86     73835\n",
            "         2.0       0.81      0.76      0.79     52387\n",
            "\n",
            "    accuracy                           0.83    126222\n",
            "   macro avg       0.83      0.82      0.82    126222\n",
            "weighted avg       0.83      0.83      0.83    126222\n",
            "\n",
            "day9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.88      0.86     72771\n",
            "         2.0       0.83      0.78      0.80     53451\n",
            "\n",
            "    accuracy                           0.84    126222\n",
            "   macro avg       0.84      0.83      0.83    126222\n",
            "weighted avg       0.84      0.84      0.84    126222\n",
            "\n",
            "day10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.88      0.87     71725\n",
            "         2.0       0.84      0.80      0.82     54497\n",
            "\n",
            "    accuracy                           0.85    126222\n",
            "   macro avg       0.85      0.84      0.84    126222\n",
            "weighted avg       0.85      0.85      0.85    126222\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWo_Do7EynIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.exceptions import DataConversionWarning\n",
        "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "\n",
        "# highest_accuracy = 0.8400474938512424\n",
        "# int_highest_accuracy = 481\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(401,1001):\n",
        "  \n",
        "    \n",
        "#   if(i%100==0):\n",
        "#     print(\"reached\",i)\n",
        "  \n",
        "  \n",
        "#   # specify the feature set, target set, the test size and random_state to select records randomly\n",
        "#   X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,5:-12], df['day10'], test_size=0.3,random_state=i)\n",
        "  \n",
        "#   # Scaling values in the feature set\n",
        "#   scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
        "#   X_train = scaling.transform(X_train)\n",
        "#   X_test = scaling.transform(X_test)\n",
        "  \n",
        "  \n",
        "#   # Create a random forest Classifier\n",
        "#   clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
        "\n",
        "#   # Train the model using the training sets\n",
        "#   clf.fit(X_train, y_train)\n",
        "\n",
        "#   # Predict the response for test dataset\n",
        "#   y_pred = clf.predict(X_test)\n",
        "  \n",
        "#   accuracy = metrics.f1_score(y_test, y_pred, average='micro')\n",
        "  \n",
        "#   if(accuracy > highest_accuracy):\n",
        "#     print(\"Accuracy for random state\",i,\"is higher than\",int_highest_accuracy,\"with\",accuracy)\n",
        "#     highest_accuracy = accuracy\n",
        "#     int_highest_accuracy = i\n",
        "    \n",
        "\n",
        "# print(\"Highest accuracy is for random state\",int_highest_accuracy,\"with\",highest_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64oHj-1Bmpw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # reindexing \n",
        "# y_test2 = y_test.reset_index()\n",
        "# del y_test2['index']\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # plotting graph\n",
        "\n",
        "# plt.plot(y_pred,'ro')\n",
        "# plt.plot(y_test2,'bo')\n",
        "# plt.xticks(np.arange(0, len(y_test2), 1000))\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}